{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "730df369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25f5b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"deepseek-r1:7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b3b23bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f80e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Edward Donner\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I’m the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "May 28, 2025\n",
      "Connecting my courses – become an LLM expert and leader\n",
      "May 18, 2025\n",
      "2025 AI Executive Briefing\n",
      "April 21, 2025\n",
      "The Complete Agentic AI Engineering Course\n",
      "January 23, 2025\n",
      "LLM Workshop – Hands-on with Agents – resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email…\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out\n",
    "\n",
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "717e11a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c41ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"The contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07600d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ac3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the Ollama function instead of OpenAI\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    messages = messages_for(website)\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "798bc8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nOkay, so I need to analyze this website called \"Home - Edward Donner\" and provide a short summary in markdown. The user has given the contents of the site, which includes sections like Home, Connect Four, Outsmart, About, Posts, Navigation, and Contact information.\\n\\nFirst, looking at the structure: there\\'s a header with navigation links to various sections and a footer with contact details. I should ignore any navigation text, so focusing on the content itself.\\n\\nThe main content is in the \"Home\" section. It introduces Edward Donner, who writes about AI, specifically LLMs (Large Language Models). He talks about being co-founder and CTO of Nebula.io, which uses AI to help people discover their potential. There\\'s a mention of having founded another startup that was acquired in 2021.\\n\\nHe also has some upcoming courses listed under \"Connect\" with him becoming an LLM expert and leader, with specific course dates in 2025. Additionally, there are posts about AI events scheduled for the same year.\\n\\nThe website seems to be centered around AI, particularly LLMs, education on that topic, and personal branding through his LinkedIn, Twitter, Facebook, etc.\\n\\nI should structure this into sections: About, Products/Courses, Events/Posts, and Contact. Each section can have a brief description without going too deep, just summarizing the key points.\\n</think>\\n\\n```markdown\\n# Edward Donner - Home\\n\\n## About\\nEdward Donner is an AI enthusiast and thought leader. He co-founded and led AI startup untapt before its acquisition in 2021. Currently, he serves as the CTO of Nebula.io, a company applying AI to enhance talent discovery and management.\\n\\n## Products/Courses\\nNebula.io offers proprietary LLMs for verticalized applications, including a matching model patented by the company. Edward provides access to his courses on becoming an LLM expert, such as \"Connecting my courses – become an LLM expert and leader,\" with sessions scheduled throughout 2025.\\n\\n## Events/Posts\\nEdward shares updates about AI-related events and educational content. Upcoming posts include topics like the 2025 AI Executive Briefing and a hands-on course on agentic AI engineering, all expected to take place in early 2025.\\n\\n## Contact\\nFor inquiries, visit his website or follow him on LinkedIn, Twitter, Facebook, or subscribe to his newsletter.\\n```'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63b73d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4566fb5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, I'm looking at this website called Home - Edward Donner. Let's break down the content.\n",
       "\n",
       "First, the main sections are Home and Connect Four. The Outsmart section is about an arena for LLMs. On the Home page, there's a brief introduction from Ed, who talks about his interests like coding, experimenting with LLMs, and DJing (though he's bad at it). He also mentions being a co-founder of Nebula.io, which uses AI to help people discover their potential.\n",
       "\n",
       "In the Posts section, there are announcements for courses starting in 2025. So these include connecting my courses—becoming an LLM expert and leader; an AI Executive Briefing course; The Complete Agentic AI Engineering Course; and an LLM Workshop with resources.\n",
       "\n",
       "The website also provides contact information: email is ed@edwarddonner.com, a link to his website at www.edwarddonner.com, social media links (LinkedIn, Twitter, Facebook), and a newsletter subscription option with an input field for email.\n",
       "\n",
       "So the summary should cover who Ed is, what he's involved in—AI startups and teaching. Then mention the courses available on the site. Don't include navigation terms like \"Connect Four\" since it's about content sections.\n",
       "</think>\n",
       "\n",
       "# Edward Donner - AI Enthusiast & Co-Founder of Nebula.io\n",
       "\n",
       "Hello! I'm Ed, a passionate AI enthusiast with expertise in experimenting with large language models and coding. I also enjoy DJing (though my skills are rusty) and exploring various hobbies.\n",
       "\n",
       "Previously, I was the co-founder and CTO at Nebula.io, where we leverage AI to help people discover their potential and achieve their goals through talent management. Before that, I founded an AI startup acquired in 2021.\n",
       "\n",
       "Nebula.io has patented its matching model and offers a platform with great customer satisfaction and media coverage. If you're interested, connect with me via email at ed@edwarddonner.com or visit my website at www.edwarddonner.com for more details.\n",
       "\n",
       "### Upcoming Courses:\n",
       "- **Connect My Courses**: Becoming an LLM Expert & Leader\n",
       "- **AI Executive Briefing**: 2025 AI Executive Course\n",
       "- **The Complete Agentic AI Engineering Course**\n",
       "- **LLM Workshop: Hands-on with Agents & Resources**\n",
       "\n",
       "Follow me on LinkedIn, Twitter, and Facebook for updates. Subscribe to my newsletter for the latest insights.\n",
       "\n",
       "Type your email here to get started!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "068b38c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I'm trying to figure out how machine learning can be used to improve the way we handle large sets of data in journalism. The user mentioned that CNN needs an ML strategy for handling big data, which makes sense because they deal with a lot of information every day.\n",
       "\n",
       "First off, I remember that machine learning can help with organizing and categorizing content. Maybe it could automatically sort articles into topics like politics, sports, or entertainment without human intervention. That would save a lot of time for journalists.\n",
       "\n",
       "Another idea is sentiment analysis. I think this is where algorithms detect the mood behind text, like whether people are happy or upset about certain events. This could help in quickly identifying trends and public sentiment on specific issues, which is useful for reporting.\n",
       "\n",
       "Clustering is another term I came across. It's when you group similar data points together. In journalism, this might mean finding articles that cover the same story but from different angles or sources. That would make it easier to compile comprehensive reports.\n",
       "\n",
       "Natural Language Processing (NLP) sounds advanced. From what I understand, NLP can extract information from text. So journalists could use it to pull out facts and statistics directly from news articles, which would speed up data extraction processes.\n",
       "\n",
       "Data visualization is a tool that turns complex data into easy-to-understand graphs and charts. If CNN uses this, they could present information more clearly, especially in stories with lots of numbers or metrics.\n",
       "\n",
       "Predictive analytics might help forecast trends based on existing data. For example, predicting election outcomes by analyzing voter behavior patterns. This could give journalists insights into what might happen before events occur.\n",
       "\n",
       "Extracting structured data from unstructured sources like news articles would be another application. It allows for easier analysis and integration of information, which is crucial when dealing with the chaos of real-time news.\n",
       "\n",
       "Automation is a big one too. If ML can automate repetitive tasks like fact-checking or keyword searching, journalists can focus more on writing and investigative reporting instead of doing these tasks manually.\n",
       "\n",
       "Personalization features could make content more engaging by tailoring stories to the reader's interests. This might increase engagement rates, which is beneficial for both readers and advertisers.\n",
       "\n",
       "Clickbait generators are a bit controversial but potentially useful if used responsibly. They could help in creating attention-grabbing headlines without crossing into misinformation.\n",
       "\n",
       "Ethical AI compliance is another aspect. As AI becomes more integrated, ensuring it doesn't spread misinformation or bias is crucial. CNN would need guidelines to govern how ML tools are used ethically.\n",
       "\n",
       "An AI newsroom sounds exciting. Imagine a system that aggregates and processes data from various sources automatically. This could free up journalists to focus on deeper reporting rather than data entry.\n",
       "\n",
       "A dashboard for real-time data analysis would be useful during events like elections or disasters, providing quick insights to inform reporters on the ground.\n",
       "\n",
       "Lastly, archiving with AI can help in organizing older content more efficiently. It might also assist in finding similar stories from the past, which is helpful for research and continuity reporting.\n",
       "\n",
       "Overall, integrating these ML techniques could make CNN's operations smoother, provide deeper insights, and improve the quality of journalism by making data handling faster and more accurate.\n",
       "</think>\n",
       "\n",
       "Integrating machine learning into CNN's data handling strategy can revolutionize how the network manages and disseminates information. Here's a structured approach leveraging various ML techniques:\n",
       "\n",
       "1. **Automated Content Categorization**: Utilize supervised learning models to classify articles into predefined categories such as politics, sports, or entertainment, enhancing efficiency for journalists.\n",
       "\n",
       "2. **Sentiment Analysis**: Implement algorithms to gauge public sentiment on specific issues, aiding in trend identification and informed reporting.\n",
       "\n",
       "3. **Clustering**: Use unsupervised learning to group similar news items by theme or source, facilitating comprehensive report compilation from diverse sources.\n",
       "\n",
       "4. **Natural Language Processing (NLP)**: Employ NLP techniques for extracting facts and statistics directly from text, accelerating data extraction processes.\n",
       "\n",
       "5. **Data Visualization Tools**: Leverage tools to transform complex data into clear visual representations, aiding in the presentation of information in digestible formats.\n",
       "\n",
       "6. **Predictive Analytics**: Apply machine learning models to forecast trends, such as election outcomes, by analyzing voter behavior patterns.\n",
       "\n",
       "7. **Structured Data Extraction**: Use NLP and ML to convert unstructured news content into structured datasets for easier analysis and integration.\n",
       "\n",
       "8. **Automation of Repetitive Tasks**: Develop systems to automate tasks like fact-checking and keyword searching, allowing journalists to focus on writing and investigative reporting.\n",
       "\n",
       "9. **Personalization Features**: Implement algorithms that tailor content to reader interests, enhancing engagement and making stories more relevant.\n",
       "\n",
       "10. **Ethical AI Compliance**: Establish guidelines to ensure AI tools do not spread misinformation or introduce bias, maintaining integrity in journalism.\n",
       "\n",
       "11. **AI-Driven Newsroom**: Create an autonomous system for aggregating and processing data across various sources, freeing journalists to focus on deeper reporting.\n",
       "\n",
       "12. **Real-Time Data Analysis Dashboard**: Build a dashboard that provides instant insights during critical events like elections or disasters, aiding reporters on the ground.\n",
       "\n",
       "13. **AI-Aided Archiving System**: Develop tools for efficient organization of older content and retrieval of similar past stories, supporting research and continuity reporting.\n",
       "\n",
       "By integrating these techniques, CNN can enhance data handling processes, improve insight delivery, and elevate the quality of journalism through faster, more accurate, and insightful reporting."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac943119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, so I need to help the user by summarizing a website they provided. The site is titled \"Home\" and seems to be from Anthropic. Let me read through the content carefully.\n",
       "\n",
       "First, I notice that there's a lot of navigation links like \"Skip to main content,\" \"Skip to footer,\" and various menu items such as Claude, Pricing, Solutions, etc. Since the user wants to ignore navigation text, I can skip over those sections.\n",
       "\n",
       "Next, looking at the content under each menu item. There are announcements about Claude models, like Opus 4 and Sonnet 4, with details on their capabilities. Then there's a section talking about Anthropic's approach to AI safety, including their research and policies like the Responsible Scaling Policy. There's also information about economic indexes and various products and services they offer.\n",
       "\n",
       "I should focus on extracting the main points: the introduction of new models, their features, AI safety initiatives, notable announcements, product offerings, and customer support aspects. I'll make sure to structure this in a clear markdown format without including navigation links or news sections that might be considered unrelated.\n",
       "</think>\n",
       "\n",
       "```markdown\n",
       "# Anthropic's AI and Research Overview\n",
       "\n",
       "**Introduction to New Models**\n",
       "- **Claude Opus 4**: The latest intelligent AI model, now available with enhanced capabilities for coding and AI agents.\n",
       "- **Claude Sonnet 4**: Another powerful model in development, expanding Claude's abilities across various applications.\n",
       "\n",
       "**AI Safety and Ethics**\n",
       "- **Anthropic's Approach to AI Safety**: Dedicated research and policy work focusing on responsible AI development with a human benefit foundation.\n",
       "- **Core Views on AI Safety**: Resources available for understanding responsible AI practices.\n",
       "- **Responsible Scaling Policy**: Details on Anthropic's commitment to ethical scaling of AI technologies.\n",
       "\n",
       "**Product and Service Offerings**\n",
       "- **Pricing Plans**: Information on Claude.ai pricing models, including features like Max, Team, and Enterprise plans.\n",
       "- **AI Agents and Applications**: Tools for creating AI-powered applications using Claude.\n",
       "- **Research and Models**: Details on various research projects, economic indexes, and AI product development.\n",
       "\n",
       "**Customer Support and Solutions**\n",
       "- **AI Agents Development**: Resources and guides for building effective AI agents.\n",
       "- **Coding and Custom Experiences**: Tools to enhance coding capabilities and create tailored AI experiences.\n",
       "\n",
       "**About Anthropic**\n",
       "- **Anthropic Academy**: A platform for learning how to build with Claude, featuring courses and resources.\n",
       "- **Customer Stories and Engineering**: Insights from customers and engineering innovations shaping future solutions.\n",
       "\n",
       "**Announcements**\n",
       "- **June 26, 2025**: Introducing Claude 4 and other updates in the AI landscape.\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
